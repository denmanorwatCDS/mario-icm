Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Previous observation shape: torch.Size([1, 3, 42, 42])
Action shape: torch.Size([1])
New observation shape: torch.Size([1, 3, 42, 42])
Quantity of classes: 12
Action shape before: torch.Size([1])
State shape: torch.Size([1, 64])
Action shape after: torch.Size([1, 12])
ICM reward: (tensor([[0.0868, 0.0775, 0.0789, 0.0842, 0.0920, 0.0768, 0.0896, 0.0813, 0.0811,
         0.0844, 0.0812, 0.0863]], device='cuda:0', grad_fn=<SoftmaxBackward0>), tensor([[-0.0955,  0.1241, -0.0033, -0.0452, -0.0067,  0.0266, -0.0225, -0.0515,
         -0.0275, -0.0841,  0.0197,  0.0649,  0.1482, -0.0024,  0.0571,  0.0268,
         -0.0016,  0.0101, -0.0298, -0.0615,  0.0669,  0.0308,  0.0940, -0.0153,
          0.0523, -0.0239,  0.0701,  0.0257,  0.0020, -0.0192,  0.0265, -0.0028,
          0.1181,  0.0058, -0.0611, -0.0115,  0.0072, -0.0764,  0.0466,  0.0531,
          0.0262,  0.0366, -0.0538, -0.0309, -0.0286, -0.0366,  0.0380, -0.1074,
          0.0141, -0.1478,  0.0391,  0.0612, -0.0154, -0.0408,  0.0233, -0.0219,
          0.1767, -0.0169, -0.0521,  0.0063,  0.1125, -0.0996,  0.0149,  0.0212]],
       device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[-1.9954e-01,  1.3186e-01, -6.5539e-04,  1.1998e-03, -8.9636e-03,
          5.3844e-02,  1.2012e-01, -1.4911e-05,  9.1724e-01,  5.2930e-01,
          1.2468e-01, -3.3471e-02,  7.8381e-03, -7.5899e-01,  3.2864e-04,
         -7.3642e-03,  3.7359e-01,  1.9104e-02, -9.2824e-03, -2.2350e-07,
         -3.1038e-01,  6.1640e-02, -6.9780e-01,  9.3668e-05,  5.4952e-02,
         -3.2617e-07, -5.8045e-05, -3.1702e-02, -8.0618e-08,  5.5413e-03,
         -1.2527e-02, -7.2807e-01, -6.0691e-02,  2.8743e-04,  1.5023e-01,
         -2.0578e-01,  1.7921e-03, -8.7518e-05, -6.1006e-03, -1.6575e-01,
          5.0824e-03, -7.1794e-05,  4.2976e-01,  2.4617e-02, -3.2697e-01,
          7.8937e-02,  2.7205e-01,  1.2312e-04,  5.5613e-01,  6.6214e-04,
          7.5524e-04, -5.3152e-01,  8.9846e-01,  9.9594e-01,  5.5107e-02,
         -1.0273e-05,  2.2326e-01,  7.1794e-01, -9.6242e-01, -2.0347e-02,
          1.9992e-03, -8.2818e-02, -3.6478e-03, -4.5648e-03]], device='cuda:0'))
Traceback (most recent call last):
  File "/home/dvasilev/mario_icm/main.py", line 38, in <module>
    model.learn(total_timesteps=10_000_000)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/a2c/a2c.py", line 203, in learn
    return super().learn(
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 262, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 181, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 162, in step
    return self.step_wait()
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_transpose.py", line 95, in step_wait
    observations, rewards, dones, infos = self.venv.step_wait()
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 43, in step_wait
    obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/monitor.py", line 94, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/dvasilev/mario_icm/Environment_wrappers/wrappers.py", line 93, in step
    print("Shape of ICM reward {}".format(intrinsic_reward.shape))
AttributeError: 'tuple' object has no attribute 'shape'