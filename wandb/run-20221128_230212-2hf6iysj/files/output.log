Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Previous observation shape: torch.Size([1, 3, 42, 42])
Action: 0
New observation shape: torch.Size([1, 3, 42, 42])
Traceback (most recent call last):
  File "/home/dvasilev/mario_icm/main.py", line 39, in <module>
    model.learn(total_timesteps=10_000_000)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/a2c/a2c.py", line 203, in learn
    return super().learn(
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 262, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 181, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 162, in step
    return self.step_wait()
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_transpose.py", line 95, in step_wait
    observations, rewards, dones, infos = self.venv.step_wait()
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 43, in step_wait
    obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/monitor.py", line 94, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/dvasilev/mario_icm/Environment_wrappers/wrappers.py", line 90, in step
    intrinsic_reward = self.motivation_model(self.prev_obs, action, torch_obs)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/dvasilev/mario_icm/ICM/ICM.py", line 90, in forward
    predicted_state = self.forward_net(const_state, action)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/dvasilev/mario_icm/ICM/ICM.py", line 18, in forward
    action = F.one_hot(action, num_classes = self.action_dim)
TypeError: one_hot(): argument 'input' (position 1) must be Tensor, not numpy.int64