Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 97       |
|    ep_rew_mean        | 220      |
| time/                 |          |
|    fps                | 46       |
|    iterations         | 100      |
|    time_elapsed       | 10       |
|    total_timesteps    | 500      |
| train/                |          |
|    entropy_loss       | -2.27    |
|    explained_variance | 0.00425  |
|    learning_rate      | 0.0007   |
|    n_updates          | 99       |
|    policy_loss        | 0.000479 |
|    value_loss         | 1.48e-07 |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 97        |
|    ep_rew_mean        | 220       |
| time/                 |           |
|    fps                | 47        |
|    iterations         | 200       |
|    time_elapsed       | 21        |
|    total_timesteps    | 1000      |
| train/                |           |
|    entropy_loss       | -2.34     |
|    explained_variance | -0.0251   |
|    learning_rate      | 0.0007    |
|    n_updates          | 199       |
|    policy_loss        | -0.000251 |
|    value_loss         | 1.29e-08  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 97       |
|    ep_rew_mean        | 220      |
| time/                 |          |
|    fps                | 46       |
|    iterations         | 300      |
|    time_elapsed       | 32       |
|    total_timesteps    | 1500     |
| train/                |          |
|    entropy_loss       | -2.37    |
|    explained_variance | 0.104    |
|    learning_rate      | 0.0007   |
|    n_updates          | 299      |
|    policy_loss        | 0.000215 |
|    value_loss         | 7.89e-09 |
------------------------------------
Traceback (most recent call last):
  File "/home/dvasilev/mario_icm/main.py", line 38, in <module>
    model.learn(total_timesteps=10_000_000)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/a2c/a2c.py", line 203, in learn
    return super().learn(
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 262, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 181, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 162, in step
    return self.step_wait()
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_transpose.py", line 95, in step_wait
    observations, rewards, dones, infos = self.venv.step_wait()
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 43, in step_wait
    obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/monitor.py", line 94, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/dvasilev/mario_icm/Environment_wrappers/wrappers.py", line 82, in step
    obs, reward, done, info = self.env.step(action)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/gym/core.py", line 323, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/atari_wrappers.py", line 139, in step
    obs, reward, done, info = self.env.step(action)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/nes_py/wrappers/joypad_space.py", line 74, in step
    return self.env.step(self._action_map[action])
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/gym/wrappers/time_limit.py", line 18, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/nes_py/nes_env.py", line 293, in step
    _LIB.Step(self._env)
KeyboardInterrupt