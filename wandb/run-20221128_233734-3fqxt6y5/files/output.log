Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Previous observation shape: torch.Size([1, 3, 42, 42])
Action shape: torch.Size([1])
New observation shape: torch.Size([1, 3, 42, 42])
Quantity of classes: 12
Action shape before: torch.Size([1])
State shape: torch.Size([1, 64])
Action shape after: torch.Size([1, 12])
ICM reward: (tensor([[0.0778, 0.0838, 0.0812, 0.0779, 0.0897, 0.0860, 0.0865, 0.0879, 0.0786,
         0.0825, 0.0861, 0.0821]], device='cuda:0', grad_fn=<SoftmaxBackward0>), tensor([[ 0.0073,  0.0613, -0.0395,  0.0371,  0.0091,  0.0329,  0.1167, -0.0264,
          0.0550,  0.0670, -0.0377,  0.0669, -0.0119, -0.0325,  0.0438,  0.0640,
          0.0736, -0.0260,  0.0084,  0.0030, -0.0596, -0.0007,  0.0608,  0.0697,
          0.0294,  0.0172,  0.0636,  0.0230, -0.0060, -0.0058,  0.0655, -0.0108,
         -0.0198, -0.0815, -0.0327,  0.0206,  0.0725,  0.0216, -0.0600,  0.0306,
          0.0042,  0.0373, -0.0067, -0.0702,  0.0543,  0.0575, -0.0300,  0.1365,
          0.0221, -0.0415,  0.0670, -0.0478, -0.0317, -0.0747, -0.0186, -0.0330,
          0.0525,  0.0196, -0.0960, -0.0311,  0.0060, -0.0164,  0.0142,  0.0096]],
       device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[-8.6231e-01, -4.9027e-03, -8.7573e-01,  4.4496e-01,  9.4640e-01,
          7.9218e-04, -1.2656e-02,  2.5969e-01, -1.0543e-01,  1.4910e-02,
          7.2729e-03, -8.7420e-01,  8.8349e-01,  4.4868e-02,  5.6160e-02,
          3.9351e-02, -1.1430e-02, -4.9992e-01,  4.4678e-01,  8.1636e-03,
          2.0425e-02, -3.1528e-02,  1.4786e-01, -9.8290e-01, -3.8688e-01,
         -2.7383e-01,  7.3433e-01, -8.8310e-01, -1.0489e-02,  6.1786e-01,
         -3.5221e-01, -7.1185e-02,  5.8683e-04,  6.0735e-01, -5.8218e-01,
         -7.9089e-01, -9.9645e-01, -6.7486e-02, -7.2732e-01, -8.0812e-01,
          5.3196e-01,  4.4095e-01,  5.7898e-02, -3.0961e-02, -8.1226e-01,
         -2.1771e-02, -9.9313e-01, -9.9183e-01,  5.3242e-04, -2.4772e-02,
         -1.5079e-03,  8.0368e-01,  1.7962e-01, -4.1186e-01, -2.4765e-02,
          9.5529e-01, -6.2219e-02, -4.6213e-02,  4.8832e-01, -4.8151e-01,
         -3.2430e-02, -1.8603e-03, -7.2310e-01, -3.3175e-01]], device='cuda:0'))
Traceback (most recent call last):
  File "/home/dvasilev/mario_icm/main.py", line 38, in <module>
    model.learn(total_timesteps=10_000_000)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/a2c/a2c.py", line 203, in learn
    return super().learn(
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 262, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 181, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 162, in step
    return self.step_wait()
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_transpose.py", line 95, in step_wait
    observations, rewards, dones, infos = self.venv.step_wait()
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 43, in step_wait
    obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(
  File "/home/dvasilev/mario_icm/.conda/lib/python3.9/site-packages/stable_baselines3/common/monitor.py", line 94, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/dvasilev/mario_icm/Environment_wrappers/wrappers.py", line 93, in step
    print("Shape of ICM reward {}".format(intrinsic_reward.shape))
AttributeError: 'tuple' object has no attribute 'shape'