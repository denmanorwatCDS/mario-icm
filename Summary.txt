1. A2C: На данный момент имеется изменённая версия работавшего ранее кода. 
На текущий момент эта версия застревает в deadlock'е. 
Сейчас проводится эксперимент с замененными моими wrapper'ами на wrapper'ы stable_baselines3. Возможно, сейчас заработает. А может, и нет

2. Книжный ICM перекопирован в VSCode, прикручены логи. Запущена колабовская версия книжного ICM'а с логами. В коде авторов найдена пара 'ошибок': двойной softmax
и неправильная константа при forward loss'е (1 вместо 1/2).
Все run'ы засижены на одинаковый SEED, обучены на одинаковом буффере. Буфер собирался с помощью агента DQN, обучаемогnо на награде ICM первые 1_000 шагов.
Размер буфера: 1_000. Потом этот буфер был скормлен всем трем запускам ICM'а (один колабовский, два VSCode'овских)
Конфиг экспериментов с.м. в CONFIG.py.

3. В моём ICM был найден ещё один баг с протечкой градиента через предсказанный action DQN'ом. Он был исправлен. Не известно, приводил ли этот баг к странным
результатам экспериментов (Совместные и раздельные ICM'ы; отличие поведения Inverse loss'а версии colab'а и VSCode), показанных в понедельник, 
или то был косяк в модификации кода авторов. В моём ICM намеренно повторены все ошибки книжного ICM для воспроизводимости результатов; места ошибок отмечены "WARNING".
Также открытым остаётся вопрос, нужно ли отключать подсчёт градиентов или нет (хоть в книге и написано, что нужно, вопрос спорный). Возможно, стоит изучить исходники
Pathak'а. Также, необходимо разобраться с нормировкой Loss'ов. По каким-то причинам, у Pathak'а большая вес отдаётся forward loss'y, а в книге больший вес отдаётся 
inverse loss'у.

4. Схема нейронок - см. файл ICM.